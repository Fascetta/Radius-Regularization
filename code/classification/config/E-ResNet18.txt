# python code/classification/train.py -c classification/config/E-ResNet18.txt

# Output settings
exp_name = E-ResNet18-RadiusLoss-CosineAnnealingLR
output_dir = classification/output

# General settings
device = cuda:0
dtype = float32
seed = 1

# Test
# load_checkpoint = classification/output/cifar-100/best_E-ResNet18.pth
# mode = calibrate
# calibration = confidence

# General training hyperparameters
num_epochs = 200
batch_size = 128
lr = 1e-1
weight_decay = 5e-4
optimizer = SGD
use_lr_scheduler = True
lr_scheduler = CosineAnnealingLR
lr_scheduler_milestones = [60,120,160]
lr_scheduler_gamma = 0.2
base_loss = cross_entropy   # cross_entropy or focal_loss
radius_loss = 1e-1

# General validation/testing hyperparameters
batch_size_test = 128
validation_split = False

# Model selection
num_layers = 18
embedding_dim = 512
encoder_manifold = euclidean
decoder_manifold = euclidean

# Dataset settings
dataset = Tiny-ImageNet  # CIFAR-10 or CIFAR-100 or Tiny-ImageNet

# Logging
wandb = True