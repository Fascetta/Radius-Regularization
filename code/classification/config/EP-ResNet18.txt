# Output settings
exp_name = EP-ResNet18_RAL-increment-ep30
notes = "Increment RAL alpha linearly throughout the training starting from 0.001 to 0.1 up to epoch 30"

output_dir = classification/output

# General settings
gpus = [1,4,5,6]
dtype = float32
seed = 1

# Test
# load_checkpoint = classification/output/cifar-100/best_L-ResNet18-RadiusLoss-mse-2.pth
# mode = calibrate
# calibration = confidence

# General training hyperparameters
num_epochs = 200
batch_size = 128
lr = 1e-1
weight_decay = 5e-4
optimizer = RiemannianSGD
use_lr_scheduler = True
lr_scheduler = MultiStepLR             # CosineAnnealingLR or MultiStepLR
lr_scheduler_milestones = [60,120,160]
lr_scheduler_gamma = 0.2
base_loss = cross_entropy               # cross_entropy or focal_loss
ral_initial_alpha = 0.001
ral_final_alpha = 0.1
radius_conf_loss = 0.
radius_label_smoothing = False

# General validation/testing hyperparameters
batch_size_test = 128
validation_split = False

# Model selection
num_layers = 18
embedding_dim = 512
encoder_manifold = euclidean
decoder_manifold = poincare

# Manifold settings
# learn_k = True
encoder_k = 1.0
decoder_k = 1.0
clip_features = 1.0

# Dataset settings
dataset = Tiny-ImageNet         # CIFAR-10 or Tiny-ImageNet

# Logging
wandb = True